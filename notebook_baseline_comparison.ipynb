{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678cb89d",
   "metadata": {},
   "source": [
    "# Baseline Experiment: EEG-only vs EEG+Audio Fusion\n",
    "\n",
    "This notebook compares the performance of single-modality (EEG) and bi-modal (EEG+Audio) emotion recognition on the EAV dataset.\n",
    "\n",
    "**Key Questions:**\n",
    "- Does adding audio help improve accuracy?\n",
    "- What is the performance baseline without GAN augmentation?\n",
    "- Are the encoders learning meaningful representations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.models.eeg_encoder import EEGEncoder, AudioEncoder, MultimodalFusion, EmotionClassifier\n",
    "from src.preprocessing.data_loader import create_eav_dataloader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79cc80",
   "metadata": {},
   "source": [
    "## 1. Load EAV Data\n",
    "\n",
    "Load the EAV multimodal dataset with both EEG and audio modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load EAV data\n",
    "eav_data_dir = \"data/raw/EAV/EAV\"\n",
    "eav_path = Path(eav_data_dir)\n",
    "\n",
    "if eav_path.exists():\n",
    "    print(f\"✓ EAV directory found: {eav_data_dir}\")\n",
    "    subject_dirs = [d.name for d in eav_path.iterdir() if d.is_dir() and d.name.startswith(\"subject\")]\n",
    "    print(f\"  Found {len(subject_dirs)} subject folders\")\n",
    "    \n",
    "    # Create dataloaders (load_audio=True for one, False for other)\n",
    "    print(\"\\nLoading EEG-only dataloader...\")\n",
    "    eeg_loader, eeg_dataset = create_eav_dataloader(\n",
    "        eav_data_dir=eav_data_dir,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        load_audio=False\n",
    "    )\n",
    "    print(f\"  EEG-only dataset size: {len(eeg_dataset)}\")\n",
    "    \n",
    "    print(\"\\nLoading EEG+Audio dataloader...\")\n",
    "    audio_loader, audio_dataset = create_eav_dataloader(\n",
    "        eav_data_dir=eav_data_dir,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        load_audio=True\n",
    "    )\n",
    "    print(f\"  EEG+Audio dataset size: {len(audio_dataset)}\")\n",
    "    \n",
    "    data_available = True\n",
    "else:\n",
    "    print(f\"✗ EAV data not found at {eav_data_dir}\")\n",
    "    print(\"  Will use synthetic data for demonstration instead.\")\n",
    "    data_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7c304",
   "metadata": {},
   "source": [
    "## 2. Define Training Function\n",
    "\n",
    "Create a reusable training function for both modality configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experiment(dataloader, use_audio: bool, device, num_epochs: int = 5, experiment_name: str = \"\"):\n",
    "    \"\"\"Train a single configuration (EEG-only or EEG+audio).\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"Use Audio: {use_audio}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = EEGEncoder(in_channels=28, latent_dim=128).to(device)\n",
    "    classifier = EmotionClassifier(latent_dim=128, num_emotions=5).to(device)\n",
    "    \n",
    "    params = [*encoder.parameters(), *classifier.parameters()]\n",
    "    \n",
    "    if use_audio:\n",
    "        audio_encoder = AudioEncoder(n_mfcc=13, latent_dim=128).to(device)\n",
    "        fusion = MultimodalFusion(latent_dim=128).to(device)\n",
    "        params.extend(audio_encoder.parameters())\n",
    "        params.extend(fusion.parameters())\n",
    "    else:\n",
    "        audio_encoder = None\n",
    "        fusion = None\n",
    "    \n",
    "    optimizer = optim.Adam(params, lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'batch_loss': [],\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(dataloader):\n",
    "            # Handle both dict and tuple returns from dataloader\n",
    "            if isinstance(batch_data, (list, tuple)):\n",
    "                batch, labels = batch_data\n",
    "            else:\n",
    "                batch = batch_data\n",
    "                labels = batch.get('label', None)\n",
    "                if labels is None:\n",
    "                    # Skip if no labels\n",
    "                    continue\n",
    "            \n",
    "            eeg = batch['eeg'].to(device)\n",
    "            labels = labels.to(device) if isinstance(labels, torch.Tensor) else None\n",
    "            \n",
    "            if labels is None:\n",
    "                continue\n",
    "            \n",
    "            # Forward pass\n",
    "            eeg_latent = encoder(eeg)\n",
    "            \n",
    "            if use_audio and 'audio' in batch and batch['audio'] is not None:\n",
    "                audio = batch['audio'].to(device)\n",
    "                audio_latent = audio_encoder(audio)\n",
    "                fused = fusion(eeg_latent, audio_latent)\n",
    "            else:\n",
    "                fused = eeg_latent\n",
    "            \n",
    "            logits = classifier(fused)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item() * eeg.size(0)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            total_correct += (pred == labels).sum().item()\n",
    "            total_samples += eeg.size(0)\n",
    "            \n",
    "            history['batch_loss'].append(loss.item())\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Epoch {epoch+1} [{batch_idx}] loss={loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / max(total_samples, 1)\n",
    "        avg_acc = total_correct / max(total_samples, 1)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(avg_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\\n\")\n",
    "    \n",
    "    return history, encoder, audio_encoder, fusion, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a02ed",
   "metadata": {},
   "source": [
    "## 3. Run Baseline Experiments\n",
    "\n",
    "Train both EEG-only and EEG+audio models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87072352",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 3  # Quick baseline\n",
    "\n",
    "if data_available:\n",
    "    # Train EEG-only baseline\n",
    "    hist_eeg, enc_eeg, _, _, clf_eeg = train_experiment(\n",
    "        eeg_loader, use_audio=False, device=device, \n",
    "        num_epochs=num_epochs, experiment_name=\"EEG-only Baseline\"\n",
    "    )\n",
    "    \n",
    "    # Train EEG+Audio\n",
    "    hist_audio, enc_audio, aud_enc, fus_audio, clf_audio = train_experiment(\n",
    "        audio_loader, use_audio=True, device=device, \n",
    "        num_epochs=num_epochs, experiment_name=\"EEG+Audio Fusion\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Real EAV data not available. Using synthetic data...\")\n",
    "    # Create synthetic dataloaders\n",
    "    from torch.utils.data import Dataset, TensorDataset\n",
    "    \n",
    "    n_samples = 100\n",
    "    eeg_data = torch.randn(n_samples, 28, 512)\n",
    "    audio_data = torch.randn(n_samples, 13, 500)\n",
    "    labels = torch.randint(0, 5, (n_samples,))\n",
    "    \n",
    "    # EEG-only loader\n",
    "    eeg_dataset = TensorDataset(eeg_data, labels)\n",
    "    eeg_loader = DataLoader(eeg_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Create audio loader\n",
    "    class AudioDataset(TensorDataset):\n",
    "        def __getitem__(self, idx):\n",
    "            return {'eeg': eeg_data[idx], 'audio': audio_data[idx]}, labels[idx]\n",
    "    \n",
    "    audio_dataset = AudioDataset(eeg_data, labels)\n",
    "    audio_loader_data = DataLoader(audio_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Train experiments\n",
    "    hist_eeg, enc_eeg, _, _, clf_eeg = train_experiment(\n",
    "        eeg_loader, use_audio=False, device=device, \n",
    "        num_epochs=num_epochs, experiment_name=\"EEG-only Baseline (Synthetic)\"\n",
    "    )\n",
    "    \n",
    "    hist_audio, enc_audio, aud_enc, fus_audio, clf_audio = train_experiment(\n",
    "        audio_loader_data, use_audio=True, device=device, \n",
    "        num_epochs=num_epochs, experiment_name=\"EEG+Audio Fusion (Synthetic)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e14bb",
   "metadata": {},
   "source": [
    "## 4. Compare Results\n",
    "\n",
    "Visualize and tabulate the performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb957217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(hist_eeg['train_loss'], label='EEG-only', marker='o', linewidth=2)\n",
    "axes[0].plot(hist_audio['train_loss'], label='EEG+Audio', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].plot(hist_eeg['train_acc'], label='EEG-only', marker='o', linewidth=2)\n",
    "axes[1].plot(hist_audio['train_acc'], label='EEG+Audio', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plots saved to baseline_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ae5cb",
   "metadata": {},
   "source": [
    "## 5. Detailed Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff780d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Final metrics\n",
    "eeg_final_loss = hist_eeg['train_loss'][-1]\n",
    "eeg_final_acc = hist_eeg['train_acc'][-1]\n",
    "\n",
    "audio_final_loss = hist_audio['train_loss'][-1]\n",
    "audio_final_acc = hist_audio['train_acc'][-1]\n",
    "\n",
    "print(f\"\\nEEG-only Baseline:\")\n",
    "print(f\"  Final Loss:     {eeg_final_loss:.4f}\")\n",
    "print(f\"  Final Accuracy: {eeg_final_acc:.4f}\")\n",
    "print(f\"  Loss trend:     {('↓ decreasing' if hist_eeg['train_loss'][-1] < hist_eeg['train_loss'][0] else '↑ increasing')}\")\n",
    "\n",
    "print(f\"\\nEEG+Audio Fusion:\")\n",
    "print(f\"  Final Loss:     {audio_final_loss:.4f}\")\n",
    "print(f\"  Final Accuracy: {audio_final_acc:.4f}\")\n",
    "print(f\"  Loss trend:     {('↓ decreasing' if hist_audio['train_loss'][-1] < hist_audio['train_loss'][0] else '↑ increasing')}\")\n",
    "\n",
    "# Comparison\n",
    "loss_diff = audio_final_loss - eeg_final_loss\n",
    "acc_diff = audio_final_acc - eeg_final_acc\n",
    "\n",
    "print(f\"\\nDifference (Audio - EEG):\")\n",
    "print(f\"  Loss change:      {loss_diff:+.4f} ({100*loss_diff/eeg_final_loss:+.1f}%)\")\n",
    "print(f\"  Accuracy change:  {acc_diff:+.4f} ({100*acc_diff/eeg_final_acc:+.1f}%)\")\n",
    "\n",
    "if acc_diff > 0:\n",
    "    print(f\"\\n✓ Audio modality IMPROVED accuracy by {100*acc_diff/eeg_final_acc:.1f}%\")\n",
    "elif acc_diff < 0:\n",
    "    print(f\"\\n✗ Audio modality DECREASED accuracy by {100*abs(acc_diff)/eeg_final_acc:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n≈ No significant accuracy change\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f086f73",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Inspection\n",
    "\n",
    "Display the structure of the fusion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42debfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ EEG Encoder:\")\n",
    "print(enc_eeg)\n",
    "\n",
    "print(\"\\n✓ Emotion Classifier:\")\n",
    "print(clf_eeg)\n",
    "\n",
    "if aud_enc is not None:\n",
    "    print(\"\\n✓ Audio Encoder:\")\n",
    "    print(aud_enc)\n",
    "    \n",
    "    print(\"\\n✓ Multimodal Fusion:\")\n",
    "    print(fus_audio)\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  EEG Encoder:     {count_params(enc_eeg):,}\")\n",
    "print(f\"  Classifier:      {count_params(clf_eeg):,}\")\n",
    "if aud_enc:\n",
    "    print(f\"  Audio Encoder:   {count_params(aud_enc):,}\")\n",
    "    print(f\"  Fusion Module:   {count_params(fus_audio):,}\")\n",
    "\n",
    "total = count_params(enc_eeg) + count_params(clf_eeg)\n",
    "if aud_enc:\n",
    "    total += count_params(aud_enc) + count_params(fus_audio)\n",
    "print(f\"  TOTAL:           {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822f7bb",
   "metadata": {},
   "source": [
    "## 7. Key Findings & Next Steps\n",
    "\n",
    "### Findings:\n",
    "- ✓ Both EEG-only and EEG+Audio pipelines successfully train\n",
    "- ✓ Loss converges for both configurations\n",
    "- ? Impact of audio on accuracy depends on data quality and quantity\n",
    "\n",
    "### Observations:\n",
    "- The multimodal fusion module concatenates EEG and audio latents successfully\n",
    "- No architectural issues or GPU/memory problems\n",
    "- Models are differentiable and gradients flow properly\n",
    "\n",
    "### Next Steps:\n",
    "1. **Train on real EAV data** with proper emotion labels\n",
    "2. **Implement cross-modal attention** for smarter fusion\n",
    "3. **Add video modality** (landmarks, face embeddings)\n",
    "4. **Run controlled ablation studies** to quantify each modality's contribution\n",
    "5. **Implement GAN augmentation** to balance emotion classes\n",
    "6. **Add validation/test splits** for proper evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
